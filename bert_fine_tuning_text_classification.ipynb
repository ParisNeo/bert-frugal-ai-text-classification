{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers[torch] datasets torch scikit-learn accelerate>=0.26.0 lollms_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT for Multi-Class Text Classification\n",
    "In this notebook, we will fine-tune a pre-trained BERT model for the text classification task in the Frugal AI Challenge. The steps include:\n",
    "1. Loading the dataset.\n",
    "2. Preprocessing the text data.\n",
    "3. Adding a custom classification head to BERT.\n",
    "4. Fine-tuning the model.\n",
    "5. Evaluating the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"QuotaClimat/frugalaichallenge-text-train\", \"default\")\n",
    "\n",
    "# Display the dataset structure\n",
    "print(dataset)\n",
    "\n",
    "# Split the training set into train and validation subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert the training set to a Pandas DataFrame for splitting\n",
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "\n",
    "# Drop the duplicate column \"__index_level_0__\" if it exists\n",
    "if \"__index_level_0__\" in train_df.columns:\n",
    "    train_df = train_df.drop(columns=[\"__index_level_0__\"])\n",
    "\n",
    "# Define the label text and organize them by their prefixes\n",
    "label_texts = [\n",
    "    \"0_not_relevant\",\n",
    "    \"1_not_happening\",\n",
    "    \"2_not_human\",\n",
    "    \"3_not_bad\",\n",
    "    \"4_solutions_harmful_unnecessary\",\n",
    "    \"5_science_unreliable\",\n",
    "    \"6_proponents_biased\",\n",
    "    \"7_fossil_fuels_needed\"\n",
    "]\n",
    "\n",
    "# Sort the labels alphabetically by their prefix\n",
    "sorted_labels = sorted(label_texts, key=lambda x: x.split(\"_\")[0])\n",
    "\n",
    "# Create a mapping from label text to integers based on the sorted order\n",
    "label_mapping = {label: idx for idx, label in enumerate(sorted_labels)}\n",
    "\n",
    "# Map the labels in the DataFrame\n",
    "train_df[\"label\"] = train_df[\"label\"].map(label_mapping)\n",
    "\n",
    "# Perform an 80-20 split for training and validation\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.2, stratify=train_df[\"label\"], random_state=42)\n",
    "\n",
    "# Convert the split data back to Hugging Face Dataset format\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# Update the dataset dictionary to include the validation set\n",
    "dataset = {\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "}\n",
    "\n",
    "# Display the updated dataset structure\n",
    "print(dataset)\n",
    "\n",
    "# Display the label mapping for reference\n",
    "print(\"Label Mapping:\", label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lollms_client import LollmsClient\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Initialize LollmsClient\n",
    "lc = LollmsClient(\"http://localhost:9600\")\n",
    "\n",
    "# Check class distribution\n",
    "class_counts = train_df[\"label\"].value_counts()\n",
    "print(\"Class Distribution Before Balancing:\")\n",
    "print(class_counts)\n",
    "\n",
    "# Determine the target number of examples per class (based on the majority class)\n",
    "target_count = class_counts.max()\n",
    "\n",
    "import random\n",
    "\n",
    "# Function to generate additional examples for a class\n",
    "def generate_examples_for_class(class_name, existing_examples, num_examples_needed):\n",
    "    # Randomly select up to 10 examples from the existing examples\n",
    "    random_examples = random.sample(existing_examples, min(len(existing_examples), 10))\n",
    "    \n",
    "    # Prepare the prompt with the randomly selected examples\n",
    "    example_texts = \",\\n\".join([f'\"{text}\"' for text in random_examples])  # Use up to 10 random examples for the prompt\n",
    "    batch_size = 10  # Generate 10 examples at a time\n",
    "    \n",
    "    # Build the JSON structure as a string\n",
    "    json_structure = (\n",
    "        \"{\\n\"\n",
    "        '    \"class\": \"' + class_name + '\",\\n'\n",
    "        '    \"examples\": [\\n'\n",
    "        + example_texts + \"\\n\"\n",
    "        \"    ]\\n\"\n",
    "        \"}\"\n",
    "    )\n",
    "    \n",
    "    # Build the full prompt\n",
    "    prompt_template = (\n",
    "        \"Build a JSON code that contains a list of new text examples in the same class: \"\n",
    "        + class_name\n",
    "        + \".\\nHere are some examples from the class:\\n```json\\n\"\n",
    "        + json_structure\n",
    "        + \"\\n```\\n\\n\"\n",
    "        + \"Generate \"\n",
    "        + str(batch_size)\n",
    "        + \" new examples in the same style and tone.\"\n",
    "    )\n",
    "    \n",
    "    generated_examples = []\n",
    "    while num_examples_needed > 0:\n",
    "        current_batch_size = min(batch_size, num_examples_needed)  # Adjust batch size for the remaining examples\n",
    "        prompt = prompt_template.replace(str(batch_size), str(current_batch_size))  # Update batch size in the prompt\n",
    "        \n",
    "        # Debug: Print the prompt\n",
    "        print(\"Prompt being sent:\")\n",
    "        print(prompt)\n",
    "        \n",
    "        # Generate synthetic examples using LollmsClient\n",
    "        response = lc.generate_code(prompt)\n",
    "        \n",
    "        # Debug: Print the response\n",
    "        print(\"Response received:\")\n",
    "        print(response)\n",
    "        \n",
    "        # Parse the generated JSON\n",
    "        try:\n",
    "            generated_data = json.loads(response.strip())  # Parse the JSON response\n",
    "            if \"examples\" in generated_data:\n",
    "                generated_examples.extend(generated_data[\"examples\"])\n",
    "                num_examples_needed -= len(generated_data[\"examples\"])\n",
    "            else:\n",
    "                print(f\"Unexpected response format: {response}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON response for class {class_name}: {e}\")\n",
    "            break  # Exit the loop if there's an error\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError in response for class {class_name}: {e}\")\n",
    "            break  # Exit the loop if there's an error\n",
    "    \n",
    "    return generated_examples\n",
    "from tqdm.notebook import tqdm  # Import tqdm for progress bar\n",
    "import random\n",
    "\n",
    "# Generate additional examples for underrepresented classes\n",
    "new_data = []\n",
    "\n",
    "for label, count in tqdm(class_counts.items(), desc=\"Balancing Classes\"):\n",
    "    if count < target_count:\n",
    "        class_name = [key for key, value in label_mapping.items() if value == label][0]  # Get the class name\n",
    "        num_examples_needed = target_count - count\n",
    "        existing_examples = train_df[train_df[\"label\"] == label][\"quote\"].tolist()\n",
    "        \n",
    "        # Debug: Check existing examples\n",
    "        print(f\"Class: {class_name}, Existing Examples: {len(existing_examples)}\")\n",
    "        \n",
    "        # Randomly select up to 10 examples\n",
    "        random_examples = random.sample(existing_examples, min(len(existing_examples), 10))\n",
    "        \n",
    "        # Generate new examples\n",
    "        generated_examples = generate_examples_for_class(class_name, random_examples, num_examples_needed)\n",
    "        \n",
    "        # Debug: Check generated examples\n",
    "        print(f\"Class: {class_name}, Needed: {num_examples_needed}, Generated: {len(generated_examples)}\")\n",
    "        \n",
    "        # Add the generated examples to the new data\n",
    "        for example in generated_examples:\n",
    "            new_data.append({\"quote\": example, \"label\": label})\n",
    "\n",
    "# Convert the new data to a DataFrame\n",
    "new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Debug: Check new data shape\n",
    "print(f\"New Data Shape: {new_data_df.shape}\")\n",
    "\n",
    "# Append the new data to the training DataFrame\n",
    "balanced_train_df = pd.concat([train_df, new_data_df], ignore_index=True)\n",
    "\n",
    "# Debug: Check final distribution\n",
    "print(\"Class Distribution After Balancing:\")\n",
    "print(balanced_train_df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class Distribution After Balancing:\")\n",
    "print(balanced_train_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the target number of examples per class (based on the majority class)\n",
    "target_count = class_counts.max()\n",
    "\n",
    "print(balanced_train_df)\n",
    "# Convert the balanced DataFrame back to Hugging Face Dataset format\n",
    "balanced_train_dataset = Dataset.from_pandas(balanced_train_df)\n",
    "\n",
    "# Update the dataset dictionary\n",
    "dataset[\"train\"] = balanced_train_dataset\n",
    "\n",
    "# Check the new class distribution\n",
    "balanced_class_counts = balanced_train_df[\"label\"].value_counts()\n",
    "print(\"Class Distribution After Balancing:\")\n",
    "print(balanced_class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated dataset to a local directory\n",
    "balanced_train_dataset.save_to_disk(\"balanced_train_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Load the dataset dictionary from the saved directory\n",
    "dataset = DatasetDict.load_from_disk(\"balanced_dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class weight\n",
    "This database is not balanced, so we need to try using a technique to solve this problem. Here we decided to do class weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = balanced_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Get the labels from the training dataset\n",
    "train_labels = train_df[\"label\"].values\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "# Convert class weights to a PyTorch tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "print(\"Class Weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the text data\n",
    "def preprocess_data(examples):\n",
    "    return tokenizer(examples[\"quote\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply the tokenizer to the train and validation datasets\n",
    "tokenized_train_dataset = dataset[\"train\"].map(preprocess_data, batched=True)\n",
    "tokenized_val_dataset = dataset[\"validation\"].map(preprocess_data, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "tokenized_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model with a classification head\n",
    "num_labels = 8  # Number of unique labels in the dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Define a custom model with weighted loss\n",
    "class WeightedBERT(BertForSequenceClassification):\n",
    "    def __init__(self, config, class_weights):\n",
    "        super().__init__(config)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask, labels=labels, **kwargs)\n",
    "        if labels is not None:\n",
    "            loss_fn = CrossEntropyLoss(weight=self.class_weights.to(outputs.logits.device))\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            outputs.loss = loss\n",
    "        return outputs\n",
    "\n",
    "# Load the model with class weights\n",
    "model = WeightedBERT.from_pretrained(\"bert-base-uncased\", num_labels=len(label_mapping), class_weights=class_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments with gradient clipping\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Output directory\n",
    "    evaluation_strategy=\"epoch\",    # Evaluate every epoch\n",
    "    learning_rate=2e-5,             # Learning rate\n",
    "    per_device_train_batch_size=16, # Batch size for training\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation\n",
    "    num_train_epochs=10,            # Increased epochs to allow early stopping\n",
    "    weight_decay=0.01,              # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",           # Directory for logs\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",          # Save model at the end of each epoch\n",
    "    load_best_model_at_end=True,    # Load the best model based on validation\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to monitor for best model\n",
    "    greater_is_better=False,        # Lower eval_loss is better\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps\n",
    "    max_grad_norm=1.0               # Gradient clipping\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Add EarlyStoppingCallback\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2,  # Stop after 2 epochs without improvement\n",
    "    early_stopping_threshold=0.01  # Minimum improvement threshold\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]  # Add early stopping callback\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./bert-fine-tuned\")\n",
    "tokenizer.save_pretrained(\"./bert-fine-tuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get predictions on the validation set\n",
    "predictions = trainer.predict(tokenized_val_dataset)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = predictions.predictions.argmax(axis=-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=list(label_mapping.keys()), yticklabels=list(label_mapping.keys()))\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get predictions on the validation set\n",
    "predictions = trainer.predict(tokenized_val_dataset)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = predictions.predictions.argmax(axis=-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Compute classification metrics\n",
    "report = classification_report(true_labels, predicted_labels, target_names=list(label_mapping.keys()))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logs[\"epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the classification report to a text file\n",
    "with open(\"./bert-fine-tuned/classification_report.txt\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "# Save the training evolution plots\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(logs[\"epoch\"], logs[\"loss\"], label=\"Training Loss\", marker=\"o\")\n",
    "plt.plot(logs[\"epoch\"], logs[\"eval_loss\"], label=\"Validation Loss\", marker=\"o\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"./bert-fine-tuned/training_loss_plot.png\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(logs[\"epoch\"], logs[\"eval_accuracy\"], label=\"Validation Accuracy\", marker=\"o\", color=\"green\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"./bert-fine-tuned/validation_accuracy_plot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the confusion matrix plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=list(label_mapping.keys()), yticklabels=list(label_mapping.keys()))\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"./bert-fine-tuned/confusion_matrix.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the confusion matrix plot\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please open a console and type:\n",
    "# huggingface-cli login\n",
    "# log in to hugging face with your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_card(\n",
    "    model_dir, model_name, description, metrics_data, limitations, citation, label_mapping\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a model card and saves it as README.md in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str): Directory where the model card will be saved.\n",
    "        model_name (str): Name of the model.\n",
    "        description (str): Description of the model.\n",
    "        metrics_data (dict): Performance metrics of the model (e.g., precision, recall, F1-score, accuracy).\n",
    "        limitations (str): Limitations of the model.\n",
    "        citation (str): Citation for the model.\n",
    "        label_mapping (dict): Mapping between model output indices and class names.\n",
    "    \"\"\"\n",
    "    # Generate the metrics table dynamically\n",
    "    metrics_table = \"| Class | Precision | Recall | F1-Score | Support |\\n\"\n",
    "    metrics_table += \"|-------|-----------|--------|----------|---------|\\n\"\n",
    "    print(metrics_data)\n",
    "    for class_name, idx in label_mapping.items():\n",
    "        print(idx)\n",
    "        metrics_table += f\"| {class_name} | {metrics_data['precision'][idx]:.2f} | {metrics_data['recall'][idx]:.2f} | {metrics_data['f1'][idx]:.2f} | {metrics_data['support'][idx]} |\\n\"\n",
    "\n",
    "    # Add overall metrics\n",
    "    overall_metrics = (\n",
    "        f\"- **Overall Accuracy**: {metrics_data['accuracy']:.2f}\\n\"\n",
    "        f\"- **Macro Average**: Precision: {metrics_data['macro_precision']:.2f}, Recall: {metrics_data['macro_recall']:.2f}, F1-Score: {metrics_data['macro_f1']:.2f}\\n\"\n",
    "        f\"- **Weighted Average**: Precision: {metrics_data['weighted_precision']:.2f}, Recall: {metrics_data['weighted_recall']:.2f}, F1-Score: {metrics_data['weighted_f1']:.2f}\\n\"\n",
    "    )\n",
    "\n",
    "    # Generate the model card content\n",
    "    model_card_content = f\"\"\"\n",
    "---\n",
    "license: apache-2.0\n",
    "datasets:\n",
    "- QuotaClimat/frugalaichallenge-text-train\n",
    "language:\n",
    "- en\n",
    "metrics:\n",
    "- accuracy\n",
    "- f1\n",
    "base_model:\n",
    "- google-bert/bert-base-uncased\n",
    "library_name: transformers\n",
    "---\n",
    "\n",
    "# Model Card: {model_name}\n",
    "\n",
    "## Model Overview\n",
    "{description}\n",
    "\n",
    "## Dataset\n",
    "- **Source**: Frugal AI Challenge Text Task Dataset\n",
    "- **Classes**: {len(label_mapping)} unique labels representing various categories of text\n",
    "- **Preprocessing**: Tokenization using `BertTokenizer` with padding and truncation to a maximum sequence length of 128.\n",
    "\n",
    "## Model Architecture\n",
    "- **Base Model**: `bert-base-uncased`\n",
    "- **Classification Head**: Custom head with weighted cross-entropy loss to handle class imbalance.\n",
    "- **Number of Labels**: {len(label_mapping)}\n",
    "\n",
    "## Training Details\n",
    "- **Optimizer**: AdamW\n",
    "- **Learning Rate**: 2e-5\n",
    "- **Batch Size**: 16 (for both training and evaluation)\n",
    "- **Epochs**: 3\n",
    "- **Weight Decay**: 0.01\n",
    "- **Evaluation Strategy**: Performed at the end of each epoch\n",
    "- **Hardware**: Trained on GPUs for efficient computation\n",
    "\n",
    "## Performance Metrics (Validation Set)\n",
    "The following metrics were computed on the validation set (not the test set, which remains private for the competition):\n",
    "\n",
    "{metrics_table}\n",
    "\n",
    "{overall_metrics}\n",
    "\n",
    "## Training Evolution\n",
    "### Training and Validation Loss\n",
    "The training and validation loss evolution over epochs is shown below:\n",
    "\n",
    "![Training Loss](./training_loss_plot.png)\n",
    "\n",
    "### Validation Accuracy\n",
    "The validation accuracy evolution over epochs is shown below:\n",
    "\n",
    "![Validation Accuracy](./validation_accuracy_plot.png)\n",
    "\n",
    "## Confusion Matrix\n",
    "The confusion matrix below illustrates the model's performance on the validation set, highlighting areas of strength and potential misclassifications:\n",
    "\n",
    "![Confusion Matrix](./confusion_matrix.png)\n",
    "\n",
    "## Key Features\n",
    "- **Class Weighting**: Addressed dataset imbalance by incorporating class weights during training.\n",
    "- **Custom Loss Function**: Used weighted cross-entropy loss for better handling of underrepresented classes.\n",
    "- **Evaluation Metrics**: Accuracy, precision, recall, and F1-score were computed to provide a comprehensive understanding of the model's performance.\n",
    "\n",
    "## Class Mapping\n",
    "The mapping between model output indices and class names is as follows:\n",
    "{', '.join([f\"{idx}: {class_name}\" for idx, class_name in label_mapping.items()])}\n",
    "\n",
    "## Usage\n",
    "This model can be used for multi-class text classification tasks where the input text needs to be categorized into one of the eight predefined classes. It is particularly suited for datasets with class imbalance, thanks to its weighted loss function.\n",
    "\n",
    "### Example Usage\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"{model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{model_name}\")\n",
    "\n",
    "# Tokenize input text\n",
    "text = \"Your input text here\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(**inputs)\n",
    "predicted_class = outputs.logits.argmax(-1).item()\n",
    "\n",
    "print(f\"Predicted Class: {{predicted_class}}\")\n",
    "```\n",
    "\n",
    "## Limitations\n",
    "{limitations}\n",
    "\n",
    "## Citation\n",
    "{citation}\n",
    "\n",
    "## Acknowledgments\n",
    "Special thanks to the Frugal AI Challenge organizers for providing the dataset and fostering innovation in AI research.\n",
    "    \"\"\"\n",
    "    # Save the model card as README.md\n",
    "    with open(f\"{model_dir}/README.md\", \"w\") as f:\n",
    "        f.write(model_card_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Assuming these are your metrics after training\n",
    "metrics_data = {\n",
    "    'precision': {\n",
    "        0: trainer.evaluate()['eval_precision'],  # Add class-wise precision\n",
    "        # Add more class-specific metrics\n",
    "    },\n",
    "    'recall': {\n",
    "        0: trainer.evaluate()['eval_recall'],  # Add class-wise recall\n",
    "        # Add more class-specific metrics\n",
    "    },\n",
    "    'f1': {\n",
    "        0: trainer.evaluate()['eval_f1'],  # Add class-wise f1\n",
    "        # Add more class-specific metrics\n",
    "    },\n",
    "    'support': {\n",
    "        0: len(tokenized_val_dataset),  # Add class-wise support\n",
    "        # Add more class-specific metrics\n",
    "    },\n",
    "    'accuracy': trainer.evaluate()['eval_accuracy'],\n",
    "    'macro_precision': trainer.evaluate()['eval_precision'],\n",
    "    'macro_recall': trainer.evaluate()['eval_recall'],\n",
    "    'macro_f1': trainer.evaluate()['eval_f1'],\n",
    "    'weighted_precision': trainer.evaluate()['eval_precision'],\n",
    "    'weighted_recall': trainer.evaluate()['eval_recall'],\n",
    "    'weighted_f1': trainer.evaluate()['eval_f1']\n",
    "}\n",
    "\n",
    "# Create your label mapping\n",
    "model_dir = Path(\"./results\")\n",
    "\n",
    "generate_model_card(\n",
    "    model_dir=model_dir,\n",
    "    model_name=\"bert-frugal-ai-text-classification \",\n",
    "    description=\"\"\"This model implements a novel approach to handling class imbalance in text classification \n",
    "    by utilizing Large Language Models (LLMs) for data rebalancing. The base architecture uses BERT with \n",
    "    custom modifications for handling imbalanced datasets. The model employs early stopping, gradient clipping, \n",
    "    and weighted cross-entropy loss to optimize performance.\"\"\",\n",
    "    metrics_data=metrics_data,\n",
    "    limitations=\"\"\"- Performance may vary on extremely imbalanced datasets\n",
    "    - Requires significant computational resources for training\n",
    "    - Model performance is dependent on the quality of LLM-generated balanced data\n",
    "    - May not perform optimally on very long text sequences (>128 tokens)\"\"\",\n",
    "    citation=\"\"\"If you use this model, please cite:\n",
    "    @article{your_name2024llmrebalanced,\n",
    "        title={LLM-Rebalanced Transformer for Improved Text Classification},\n",
    "        author={Your Name},\n",
    "        year={2024},\n",
    "        journal={Preprint}\n",
    "    }\"\"\",\n",
    "    label_mapping=label_mapping\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./bert-fine-tuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bert-fine-tuned\")\n",
    "\n",
    "# Push the model to Hugging Face\n",
    "model.push_to_hub(\"ParisNeo/bert-frugal-ai-text-classification\")\n",
    "tokenizer.push_to_hub(\"ParisNeo/bert-frugal-ai-text-classification\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lollms_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
