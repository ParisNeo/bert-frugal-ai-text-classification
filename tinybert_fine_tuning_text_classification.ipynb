{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers[torch] datasets torch scikit-learn accelerate>=0.26.0 lollms_client onnxruntime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "max_retry = 3\n",
    "batch_size = 10  # Generate 10 examples at a time for data augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning TinyBERT for Multi-Class Text Classification\n",
    "In this notebook, we will fine-tune a pre-trained BERT model for the text classification task in the Frugal AI Challenge. The steps include:\n",
    "1. Loading the dataset.\n",
    "2. Preprocessing the text data.\n",
    "3. Adding a custom classification head to BERT.\n",
    "4. Fine-tuning the model.\n",
    "5. Evaluating the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"QuotaClimat/frugalaichallenge-text-train\", \"default\")\n",
    "\n",
    "# Display the dataset structure\n",
    "print(dataset)\n",
    "\n",
    "# Split the training set into train and validation subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert the training set to a Pandas DataFrame for splitting\n",
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "\n",
    "# Drop the duplicate column \"__index_level_0__\" if it exists\n",
    "if \"__index_level_0__\" in train_df.columns:\n",
    "    train_df = train_df.drop(columns=[\"__index_level_0__\"])\n",
    "\n",
    "# Define the label text and organize them by their prefixes\n",
    "label_texts = [\n",
    "    \"0_not_relevant\",\n",
    "    \"1_not_happening\",\n",
    "    \"2_not_human\",\n",
    "    \"3_not_bad\",\n",
    "    \"4_solutions_harmful_unnecessary\",\n",
    "    \"5_science_unreliable\",\n",
    "    \"6_proponents_biased\",\n",
    "    \"7_fossil_fuels_needed\"\n",
    "]\n",
    "\n",
    "# Sort the labels alphabetically by their prefix\n",
    "sorted_labels = sorted(label_texts, key=lambda x: x.split(\"_\")[0])\n",
    "\n",
    "# Create a mapping from label text to integers based on the sorted order\n",
    "label_mapping = {label: idx for idx, label in enumerate(sorted_labels)}\n",
    "\n",
    "# Map the labels in the DataFrame\n",
    "train_df[\"label\"] = train_df[\"label\"].map(label_mapping)\n",
    "\n",
    "# Perform an 60-20-20 split for training, validation and test\n",
    "train_data, test_data = train_test_split(train_df, test_size=0.2, stratify=train_df[\"label\"], random_state=random_state)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.1, stratify=train_data[\"label\"], random_state=random_state)\n",
    "\n",
    "# Convert the split data back to Hugging Face Dataset format\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Update the dataset dictionary to include the validation set\n",
    "dataset = {\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "}\n",
    "\n",
    "# Display the updated dataset structure\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "# remove the index from the label:\n",
    "label_mapping = {key.split(\"_\", 1)[1]: value for key, value in label_mapping.items()}\n",
    "\n",
    "# Display the label mapping for reference\n",
    "print(\"Label Mapping:\", label_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting and balancing database\n",
    "Here we use lollms to generate new examples to balance and augment the databse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lollms_client import LollmsClient\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Initialize LollmsClient\n",
    "lc = LollmsClient(\"http://localhost:9600\")\n",
    "\n",
    "# Function to generate additional examples for a class\n",
    "def generate_examples_for_class(class_name, existing_examples, num_examples_needed):\n",
    "    generated_examples = []\n",
    "    while num_examples_needed > 0:\n",
    "        current_batch_size = min(batch_size, num_examples_needed)  # Adjust batch size for the remaining examples\n",
    "        # Parse the generated JSON\n",
    "        generated_data = None\n",
    "        retry = 0\n",
    "        while not generated_data and retry<max_retry:\n",
    "            # Randomly select up to 10 examples from the existing examples\n",
    "            random_examples = random.sample(existing_examples, min(len(existing_examples), 10))\n",
    "            \n",
    "            # Prepare the prompt with the randomly selected examples\n",
    "            example_texts = \",\\n\".join([f'\"{text}\"' for text in random_examples])  # Use up to 10 random examples for the prompt\n",
    "            \n",
    "            # Build the JSON structure as a string\n",
    "            json_structure = (\n",
    "                \"{\\n\"\n",
    "                '    \"class\": \"' + class_name + '\",\\n'\n",
    "                '    \"examples\": [\\n'\n",
    "                + example_texts + \"\\n\"\n",
    "                \"    ]\\n\"\n",
    "                \"}\"\n",
    "            )\n",
    "            \n",
    "            # Build the full prompt\n",
    "            prompt_template = (\n",
    "                \"Build a JSON code that contains a list of new text examples in the same class: \"\n",
    "                + class_name\n",
    "                + \".\\nHere are some examples from the class:\\n```json\\n\"\n",
    "                + json_structure\n",
    "                + \"\\n```\\n\\n\"\n",
    "                + \"Generate \"\n",
    "                + str(batch_size)\n",
    "                + \" new examples in the same style and tone.\"\n",
    "            )\n",
    "            prompt = prompt_template.replace(str(batch_size), str(current_batch_size))  # Update batch size in the prompt\n",
    "                        \n",
    "            # Generate synthetic examples using LollmsClient\n",
    "            response = lc.generate_code(prompt)\n",
    "            if response:\n",
    "                try:\n",
    "                    generated_data = json.loads(response.strip())  # Parse the JSON response\n",
    "                    if \"examples\" in generated_data:\n",
    "                        generated_examples.extend(generated_data[\"examples\"])\n",
    "                        num_examples_needed -= len(generated_data[\"examples\"])\n",
    "                    else:\n",
    "                        print(f\"Unexpected response format: {response}\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON response for class {class_name}: {e}\")\n",
    "                except KeyError as e:\n",
    "                    print(f\"KeyError in response for class {class_name}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Exception: {e}\")\n",
    "                retry+=1\n",
    "        if retry>=max_retry:\n",
    "            print(\"Warning: Max retries reached with no success\")\n",
    "    return generated_examples\n",
    "\n",
    "# Function to augment and balance the dataset\n",
    "def augment_and_balance_dataset(train_df, label_mapping, augmentation_coefficient=1.0):\n",
    "    # Check class distribution\n",
    "    class_counts = train_df[\"label\"].value_counts()\n",
    "    print(\"Class Distribution Before Augmentation and Balancing:\")\n",
    "    print(class_counts)\n",
    "\n",
    "    # Determine the target number of examples per class\n",
    "    target_count = int(class_counts.max() * augmentation_coefficient)\n",
    "\n",
    "    # Generate additional examples for all classes\n",
    "    new_data = []\n",
    "    first = True\n",
    "    total_classes = len(class_counts)\n",
    "    for label, count in tqdm(class_counts.items(), desc=\"Augmenting and Balancing Classes\", total=total_classes):\n",
    "        class_name = [key for key, value in label_mapping.items() if value == label][0]  # Get the class name\n",
    "        num_examples_needed = target_count - count\n",
    "        existing_examples = train_df[train_df[\"label\"] == label][\"quote\"].tolist()\n",
    "        \n",
    "        # Debug: Check existing examples\n",
    "        if first:\n",
    "            print(f\"Class: {class_name}, Existing Examples: {len(existing_examples)}\")\n",
    "            first = False\n",
    "        \n",
    "        # Randomly select up to 10 examples\n",
    "        random_examples = random.sample(existing_examples, min(len(existing_examples), 10))\n",
    "        \n",
    "        # Generate new examples\n",
    "        generated_examples = generate_examples_for_class(class_name, random_examples, num_examples_needed)\n",
    "        \n",
    "        # Debug: Check generated examples\n",
    "        print(f\"Class: {class_name}, Needed: {num_examples_needed}, Generated: {len(generated_examples)}\")\n",
    "        \n",
    "        # Add the generated examples to the new data\n",
    "        for example in generated_examples:\n",
    "            new_data.append({\"quote\": example, \"label\": label, \"source\": \"lollms\", \"url\":\"\", \"language\":\"en\", \"subsource\":\"\",\"id\":0,'__index_level_0__':0})\n",
    "\n",
    "    # Convert the new data to a DataFrame\n",
    "    new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Debug: Check new data shape\n",
    "    print(f\"New Data Shape: {new_data_df.shape}\")\n",
    "\n",
    "    # Append the new data to the training DataFrame\n",
    "    augmented_balanced_train_df = pd.concat([train_df, new_data_df], ignore_index=True)\n",
    "\n",
    "    # Debug: Check final distribution\n",
    "    print(\"Class Distribution After Augmentation and Balancing:\")\n",
    "    print(augmented_balanced_train_df[\"label\"].value_counts())\n",
    "\n",
    "    return augmented_balanced_train_df\n",
    "\n",
    "# Example usage\n",
    "# Assuming `train_df` is your training DataFrame and `label_mapping` is a dictionary mapping class names to labels\n",
    "augmentation_coefficient = 1.1  # Example coefficient\n",
    "augmented_balanced_train_df = augment_and_balance_dataset(train_df, label_mapping, augmentation_coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix bad entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a fix if some entries failed to be added correctly\n",
    "# Function to process each entry in the 'quote' column\n",
    "def process_quote_entry(entry):\n",
    "    if isinstance(entry, dict):  # Check if the entry is a dictionary\n",
    "        # Extract 'text' if it exists, otherwise 'example', otherwise 'Unknown'\n",
    "        return entry.get('text') or entry.get('example') or \"Unknown\"\n",
    "    return entry  # Leave non-dictionary entries unchanged\n",
    "\n",
    "# Apply the function to the 'quote' column\n",
    "augmented_balanced_train_df['quote'] = augmented_balanced_train_df['quote'].apply(process_quote_entry)\n",
    "\n",
    "# Verify the changes\n",
    "print(augmented_balanced_train_df['quote'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "# Determine the target number of examples per class (based on the majority class)\n",
    "class_counts = train_df[\"label\"].value_counts()\n",
    "target_count = class_counts.max()\n",
    "\n",
    "print(augmented_balanced_train_df)\n",
    "# Convert the balanced DataFrame back to Hugging Face Dataset format\n",
    "balanced_train_dataset = Dataset.from_pandas(augmented_balanced_train_df)\n",
    "\n",
    "# Update the dataset dictionary\n",
    "dataset[\"train\"] = balanced_train_dataset\n",
    "print(dataset)\n",
    "balanced_dataset = DatasetDict(dataset)\n",
    "\n",
    "# Check the new class distribution\n",
    "balanced_class_counts = augmented_balanced_train_df[\"label\"].value_counts()\n",
    "print(\"Class Distribution After Balancing:\")\n",
    "print(balanced_class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated dataset to a local directory\n",
    "balanced_dataset.save_to_disk(\"./data/balanced_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Load the dataset dictionary from the saved directory\n",
    "dataset = DatasetDict.load_from_disk(\"./data/balanced_dataset\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", num_labels=len(label_mapping))\n",
    "\n",
    "# Tokenize the text data\n",
    "def preprocess_data(examples):\n",
    "    return tokenizer(examples[\"quote\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply the tokenizer to the train and validation datasets\n",
    "tokenized_train_dataset = dataset[\"train\"].map(preprocess_data, batched=True)\n",
    "tokenized_val_dataset = dataset[\"validation\"].map(preprocess_data, batched=True)\n",
    "tokenized_test_dataset = dataset[\"test\"].map(preprocess_data, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "tokenized_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments with gradient clipping\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Output directory\n",
    "    evaluation_strategy=\"epoch\",    # Evaluate every epoch\n",
    "    learning_rate=2e-5,             # Learning rate\n",
    "    per_device_train_batch_size=16, # Batch size for training\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation\n",
    "    num_train_epochs=10,            # Increased epochs to allow early stopping\n",
    "    weight_decay=0.01,              # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",           # Directory for logs\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",          # Save model at the end of each epoch\n",
    "    load_best_model_at_end=True,    # Load the best model based on validation\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to monitor for best model\n",
    "    greater_is_better=False,        # Lower eval_loss is better\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps\n",
    "    max_grad_norm=1.0               # Gradient clipping\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Add EarlyStoppingCallback\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2,  # Stop after 2 epochs without improvement\n",
    "    early_stopping_threshold=0.01  # Minimum improvement threshold\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]  # Add early stopping callback\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./models/TinyBert-fine-tuned\")\n",
    "tokenizer.save_pretrained(\"./models/TinyBert-fine-tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build some metrics graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Get predictions on the validation set\n",
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = predictions.predictions.argmax(axis=-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Compute classification metrics\n",
    "report = classification_report(true_labels, predicted_labels, target_names=list(label_mapping.keys()))\n",
    "print(report)\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the classification report to a text file\n",
    "output_dir = Path(\"./models/TinyBert-fine-tuned\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "classification_report_path = output_dir / \"classification_report.txt\"\n",
    "with classification_report_path.open(\"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "# Extract training and validation metrics\n",
    "epochs = logs[\"epoch\"]\n",
    "training_loss = logs[\"loss\"]\n",
    "validation_loss = logs[\"eval_loss\"]\n",
    "validation_accuracy = logs[\"eval_accuracy\"]\n",
    "training_accuracy = logs.get(\"accuracy\", None)  # Ensure training accuracy is logged\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, label=\"Training Loss\", marker=\"o\", linestyle=\"-\")\n",
    "plt.step(epochs, validation_loss, label=\"Validation Loss\", marker=\"o\", linestyle=\"--\", where=\"post\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "loss_plot_path = output_dir / \"training_loss_plot.png\"\n",
    "plt.savefig(loss_plot_path)\n",
    "\n",
    "# Plot Validation Accuracy and Training Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "if training_accuracy:\n",
    "    plt.plot(epochs, training_accuracy, label=\"Training Accuracy\", marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
    "plt.plot(epochs, validation_accuracy, label=\"Validation Accuracy\", marker=\"o\", linestyle=\"--\", color=\"green\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "accuracy_plot_path = output_dir / \"validation_accuracy_plot.png\"\n",
    "plt.savefig(accuracy_plot_path)\n",
    "\n",
    "print(f\"Plots saved to {output_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the confusion matrix plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=list(label_mapping.keys()), yticklabels=list(label_mapping.keys()))\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"./models/TinyBert-fine-tuned/confusion_matrix.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the confusion matrix plot\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_card(\n",
    "    model_dir, model_name, description, metrics_data, limitations, citation, label_mapping\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a model card and saves it as README.md in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str): Directory where the model card will be saved.\n",
    "        model_name (str): Name of the model.\n",
    "        description (str): Description of the model.\n",
    "        metrics_data (dict): Performance metrics of the model (e.g., precision, recall, F1-score, accuracy).\n",
    "        limitations (str): Limitations of the model.\n",
    "        citation (str): Citation for the model.\n",
    "        label_mapping (dict): Mapping between model output indices and class names.\n",
    "    \"\"\"\n",
    "    # Generate the metrics table dynamically\n",
    "    metrics_table = \"| Class | Precision | Recall | F1-Score | Support |\\n\"\n",
    "    metrics_table += \"|-------|-----------|--------|----------|---------|\\n\"\n",
    "    print(metrics_data)\n",
    "    for idx, class_name in label_mapping.items():\n",
    "        metrics_table += f\"| {class_name} | {metrics_data['precision'][idx]:.2f} | {metrics_data['recall'][idx]:.2f} | {metrics_data['f1'][idx]:.2f} | {metrics_data['support'][idx]} |\\n\"\n",
    "\n",
    "    # Add overall metrics\n",
    "    overall_metrics = (\n",
    "        f\"- **Overall Accuracy**: {metrics_data['accuracy']:.2f}\\n\"\n",
    "        f\"- **Macro Average**: Precision: {metrics_data['macro_precision']:.2f}, Recall: {metrics_data['macro_recall']:.2f}, F1-Score: {metrics_data['macro_f1']:.2f}\\n\"\n",
    "        f\"- **Weighted Average**: Precision: {metrics_data['weighted_precision']:.2f}, Recall: {metrics_data['weighted_recall']:.2f}, F1-Score: {metrics_data['weighted_f1']:.2f}\\n\"\n",
    "    )\n",
    "\n",
    "    # Generate the model card content\n",
    "    model_card_content = f\"\"\"\n",
    "---\n",
    "license: apache-2.0\n",
    "datasets:\n",
    "- QuotaClimat/frugalaichallenge-text-train\n",
    "language:\n",
    "- en\n",
    "metrics:\n",
    "- accuracy\n",
    "- f1\n",
    "base_model:\n",
    "- huawei-noah/TinyBERT_General_4L_312D\n",
    "library_name: transformers\n",
    "---\n",
    "\n",
    "# Model Card: {model_name}\n",
    "\n",
    "## Model Overview\n",
    "{description}\n",
    "\n",
    "## Dataset\n",
    "- **Source**: Frugal AI Challenge Text Task Dataset\n",
    "- **Classes**: {len(label_mapping)} unique labels representing various categories of text\n",
    "- **Preprocessing**: Tokenization using `BertTokenizer` with padding and truncation to a maximum sequence length of 128.\n",
    "\n",
    "## Model Architecture\n",
    "- **Base Model**: `huawei-noah/TinyBERT_General_4L_312D`\n",
    "- **Classification Head**: cross-entropy loss.\n",
    "- **Number of Labels**: {len(label_mapping)}\n",
    "\n",
    "## Training Details\n",
    "- **Optimizer**: AdamW\n",
    "- **Learning Rate**: 2e-5\n",
    "- **Batch Size**: 16 (for both training and evaluation)\n",
    "- **Epochs**: 3\n",
    "- **Weight Decay**: 0.01\n",
    "- **Evaluation Strategy**: Performed at the end of each epoch\n",
    "- **Hardware**: Trained on GPUs for efficient computation\n",
    "\n",
    "## Performance Metrics (Validation Set)\n",
    "The following metrics were computed on the validation set (not the test set, which remains private for the competition):\n",
    "\n",
    "{metrics_table}\n",
    "\n",
    "{overall_metrics}\n",
    "\n",
    "## Training Evolution\n",
    "### Training and Validation Loss\n",
    "The training and validation loss evolution over epochs is shown below:\n",
    "\n",
    "![Training Loss](./training_loss_plot.png)\n",
    "\n",
    "### Validation Accuracy\n",
    "The validation accuracy evolution over epochs is shown below:\n",
    "\n",
    "![Validation Accuracy](./validation_accuracy_plot.png)\n",
    "\n",
    "## Confusion Matrix\n",
    "The confusion matrix below illustrates the model's performance on the validation set, highlighting areas of strength and potential misclassifications:\n",
    "\n",
    "![Confusion Matrix](./confusion_matrix.png)\n",
    "\n",
    "## Key Features\n",
    "- **Class Weighting**: Addressed dataset imbalance by incorporating class weights during training.\n",
    "- **Custom Loss Function**: Used weighted cross-entropy loss for better handling of underrepresented classes.\n",
    "- **Evaluation Metrics**: Accuracy, precision, recall, and F1-score were computed to provide a comprehensive understanding of the model's performance.\n",
    "\n",
    "## Class Mapping\n",
    "The mapping between model output indices and class names is as follows:\n",
    "{', '.join([f\"{idx}: {class_name}\" for idx, class_name in label_mapping.items()])}\n",
    "\n",
    "## Usage\n",
    "This model can be used for multi-class text classification tasks where the input text needs to be categorized into one of the eight predefined classes. It is particularly suited for datasets with class imbalance, thanks to its weighted loss function.\n",
    "\n",
    "### Example Usage\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"{model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{model_name}\")\n",
    "\n",
    "# Tokenize input text\n",
    "text = \"Your input text here\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(**inputs)\n",
    "predicted_class = outputs.logits.argmax(-1).item()\n",
    "\n",
    "print(f\"Predicted Class: {{predicted_class}}\")\n",
    "```\n",
    "\n",
    "## Limitations\n",
    "{limitations}\n",
    "\n",
    "## Citation\n",
    "{citation}\n",
    "\n",
    "## Acknowledgments\n",
    "Special thanks to the Frugal AI Challenge organizers for providing the dataset and fostering innovation in AI research.\n",
    "    \"\"\"\n",
    "    # Save the model card as README.md\n",
    "    with open(f\"{model_dir}/README.md\", \"w\") as f:\n",
    "        f.write(model_card_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions for the validation set\n",
    "val_predictions = trainer.predict(tokenized_val_dataset)\n",
    "preds = np.argmax(val_predictions.predictions, axis=-1)\n",
    "labels = val_predictions.label_ids\n",
    "\n",
    "# Get detailed classification report\n",
    "report = classification_report(labels, preds, output_dict=True)\n",
    "\n",
    "# Create metrics data using the available results\n",
    "metrics_data = {\n",
    "    'precision': {\n",
    "        0: report['0']['precision'],\n",
    "        1: report['1']['precision'],\n",
    "        2: report['2']['precision'],\n",
    "        3: report['3']['precision'],\n",
    "        4: report['4']['precision'],\n",
    "        5: report['5']['precision'],\n",
    "        6: report['6']['precision'],\n",
    "        7: report['7']['precision']\n",
    "    },\n",
    "    'recall': {\n",
    "        0: report['0']['recall'],\n",
    "        1: report['1']['recall'],\n",
    "        2: report['2']['recall'],\n",
    "        3: report['3']['recall'],\n",
    "        4: report['4']['recall'],\n",
    "        5: report['5']['recall'],\n",
    "        6: report['6']['recall'],\n",
    "        7: report['7']['recall']\n",
    "    },\n",
    "    'f1': {\n",
    "        0: report['0']['f1-score'],\n",
    "        1: report['1']['f1-score'],\n",
    "        2: report['2']['f1-score'],\n",
    "        3: report['3']['f1-score'],\n",
    "        4: report['4']['f1-score'],\n",
    "        5: report['5']['f1-score'],\n",
    "        6: report['6']['f1-score'],\n",
    "        7: report['7']['f1-score']\n",
    "    },\n",
    "    'support': {\n",
    "        0: report['0']['support'],\n",
    "        1: report['1']['support'],\n",
    "        2: report['2']['support'],\n",
    "        3: report['3']['support'],\n",
    "        4: report['4']['support'],\n",
    "        5: report['5']['support'],\n",
    "        6: report['6']['support'],\n",
    "        7: report['7']['support']\n",
    "    },\n",
    "    'accuracy': report['accuracy'],\n",
    "    'macro_precision': report['macro avg']['precision'],\n",
    "    'macro_recall': report['macro avg']['recall'],\n",
    "    'macro_f1': report['macro avg']['f1-score'],\n",
    "    'weighted_precision': report['weighted avg']['precision'],\n",
    "    'weighted_recall': report['weighted avg']['recall'],\n",
    "    'weighted_f1': report['weighted avg']['f1-score']\n",
    "}\n",
    "\n",
    "# Updated label mapping with actual classes\n",
    "label_mapping = {\n",
    "    0: \"not_relevant\",\n",
    "    1: \"not_happening\",\n",
    "    2: \"not_human\",\n",
    "    3: \"not_bad\",\n",
    "    4: \"solutions_harmful_unnecessary\",\n",
    "    5: \"science_unreliable\",\n",
    "    6: \"proponents_biased\",\n",
    "    4: \"fossil_fuels_needed\"\n",
    "}\n",
    "\n",
    "model_dir = Path(\"./models/TinyBert-fine-tuned\")\n",
    "\n",
    "generate_model_card(\n",
    "    model_dir=model_dir,\n",
    "    model_name=\"climate-skepticism-classifier\",\n",
    "    description=\"\"\"This model implements a novel approach to classifying climate change skepticism arguments \n",
    "    by utilizing Large Language Models (LLMs) for data rebalancing. The base architecture uses BERT with \n",
    "    custom modifications for handling imbalanced datasets across 8 distinct categories of climate skepticism. \n",
    "    The model achieves exceptional performance with an accuracy of 99.92%.\n",
    "\n",
    "    The model categorizes text into the following skepticism types:\n",
    "    - Fossil fuel necessity arguments\n",
    "    - Non-relevance claims\n",
    "    - Climate change denial\n",
    "    - Anthropogenic cause denial\n",
    "    - Impact minimization\n",
    "    - Bias allegations\n",
    "    - Scientific reliability questions\n",
    "    - Solution opposition\n",
    "    \n",
    "    The unique feature of this model is its use of LLM-based data rebalancing to address the inherent class \n",
    "    imbalance in climate skepticism detection, ensuring robust performance across all argument categories.\"\"\",\n",
    "    metrics_data=metrics_data,\n",
    "    limitations=\"\"\"- Performance may vary on extremely imbalanced datasets\n",
    "    - Requires significant computational resources for training\n",
    "    - Model performance is dependent on the quality of LLM-generated balanced data\n",
    "    - May not perform optimally on very long text sequences (>128 tokens)\n",
    "    - May struggle with novel or evolving climate skepticism arguments\n",
    "    - Could be sensitive to subtle variations in argument framing\n",
    "    - May require periodic updates to capture emerging skepticism patterns\"\"\",\n",
    "    citation=\"\"\"If you use this model, please cite:\n",
    "    @article{your_name2024climateskepticism,\n",
    "        title={LLM-Rebalanced Transformer for Climate Change Skepticism Classification},\n",
    "        author={Your Name},\n",
    "        year={2024},\n",
    "        journal={Preprint}\n",
    "    }\"\"\",\n",
    "    label_mapping=label_mapping\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import torch\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./TinyBert-fine-tuned\"\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Define the ONNX export path\n",
    "onnx_path = Path(\"TinyBert-fine-tuned.onnx\")\n",
    "\n",
    "# Dummy input for tracing\n",
    "dummy_input = tokenizer(\"This is a sample input\", return_tensors=\"pt\")\n",
    "\n",
    "# Export the model to ONNX\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]),\n",
    "    onnx_path,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "    },\n",
    "    opset_version=14,  # Ensure compatibility with ONNX runtime\n",
    ")\n",
    "print(f\"Model exported to {onnx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./models/TinyBert-fine-tuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/TinyBert-fine-tuned\")\n",
    "\n",
    "# Push the model to Hugging Face\n",
    "model.push_to_hub(\"ParisNeo/TinyBert-frugal-ai-text-classification\")\n",
    "tokenizer.push_to_hub(\"ParisNeo/TinyBert-frugal-ai-text-classification\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lollms-feather",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
